{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657f8c4-3e6b-4024-8328-be6bc7e4af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b7cb1-44d6-47c7-83c9-48c5ad2a4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ebfd3-80bf-48c3-8544-6e49bbeb8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141c267-bbf2-4443-a255-e353e3524318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rag_techniques.dartboard import DartboardRag\n",
    "from research.functions import get_natural_questions_sample, get_ms_marco_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6505dc-4ec5-49f8-ba55-4fe5d1597805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DartboardRag(collection_name='natural_questions')\n",
    "df = pd.read_csv('research/data/qa_dataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e05e2-29aa-41fd-9093-8b3a93f8d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_questions_dataset = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "for i, (question, answer) in tqdm(df.loc[:200].iterrows(), total=df.loc[:200].shape[0]):\n",
    "    result = model(question)\n",
    "    natural_questions_dataset[\"question\"].append(question)\n",
    "    natural_questions_dataset[\"answer\"].append(result['answer'])\n",
    "    natural_questions_dataset[\"contexts\"].append(result['context'])\n",
    "    if pd.isna(answer):\n",
    "        answer = 'No answer'\n",
    "    natural_questions_dataset[\"ground_truth\"].append(answer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a7026-d139-4a82-8382-7f612dc1c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(natural_questions_dataset)\n",
    "new_df.to_csv('research/data/dartboard_rag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ec74f-34a9-4341-b277-a8f06afcb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.answer_correctness import AnswerCorrectness\n",
    "\n",
    "evaluator = AnswerCorrectness()\n",
    "res = evaluator.get_correctness(natural_questions_dataset['question'], natural_questions_dataset['answer'], natural_questions_dataset['ground_truth'])\n",
    "np.mean(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddd000-1f4a-4c47-b0aa-947d43356817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ragas import Evaluator\n",
    "ragas = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c88bd58-aa17-42f1-8f7d-9b33565c12ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e = ragas.eval(natural_questions_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df5e20-ae3a-49f5-9c79-14830f624a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
