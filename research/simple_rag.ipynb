{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e0d93-e4d8-4f08-92a6-278d2c8c6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277008b-0d6a-4f1f-a229-222cd923587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e1ae58-0d7a-4cbf-9337-e497dc8fcf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24cc85-68d3-402f-911a-b4ad8a333f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rag_techniques.simple_rag_giga import SimpleRag\n",
    "from research.functions import get_natural_questions_sample, get_ms_marco_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a95536-315c-4588-ab8c-91a82e0d63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ragas import Evaluator\n",
    "from src.answer_correctness import AnswerCorrectness\n",
    "ragas = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8580e-51a8-4680-b424-a64e03fa8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRag(collection_name='natural_questions')\n",
    "df = pd.read_csv('research/data/qa_dataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024e104-6d99-4db2-94de-d31d52180d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25ac1a-a9d4-4bf7-a4c9-b9103f7002bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_questions_dataset = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "for i, (question, answer) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    result = model(question)\n",
    "    natural_questions_dataset[\"question\"].append(question)\n",
    "    natural_questions_dataset[\"answer\"].append(result['answer'])\n",
    "    natural_questions_dataset[\"contexts\"].append(result['context'])\n",
    "    if pd.isna(answer):\n",
    "        answer = 'No answer'\n",
    "    natural_questions_dataset[\"ground_truth\"].append(answer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5dfe3-eaf7-4360-ab26-f72914ee60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('research/data/simple_rag_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bef95f-58a7-4d87-835c-5980924521e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = AnswerCorrectness()\n",
    "res = evaluator.get_correctness(d['question'].tolist(), d['answer'].tolist(), d['ground_truth'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fb585-444b-4b28-b76a-ea5484df1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "juge = res[1][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81dd00-89f0-431e-a1a0-53a826122296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "human = d['value'].values.tolist()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02aff9-2461-4c28-8467-09bacf928b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement = sum(e1 == e2 for e1, e2 in zip(juge, human)) / len(human)\n",
    "print(\"Доля совпадений:\", agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a32867-3621-4c68-899b-ca7252258066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(human, juge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69545a-d191-4da4-8b2e-cafa4d502296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d06c6a-7e16-4c30-b0b4-4ee2c2d68b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for j in range(5):\n",
    "    vals.append([])\n",
    "    random.shuffle(res[1])\n",
    "    for i in range(1, len(res[1])):\n",
    "        vals[j].append(np.mean(res[1][:i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb14573-8545-4305-9cf3-e9fd51673509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for val in vals:\n",
    "    plt.plot(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88729a35-2ff1-4763-8f04-9e6850c1d7bb",
   "metadata": {},
   "source": [
    "# RuLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2c540-961e-4c7c-87c0-ef618473b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRag(collection_name='natural_questions', language='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc6e0a-b56f-4ab9-bd46-8ea732ffe61c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "natural_questions_dataset = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "for i, (question, answer) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    result = model(question)\n",
    "    natural_questions_dataset[\"question\"].append(question)\n",
    "    natural_questions_dataset[\"answer\"].append(result['answer'])\n",
    "    natural_questions_dataset[\"contexts\"].append(result['context'])\n",
    "    if pd.isna(answer):\n",
    "        answer = 'No answer'\n",
    "    natural_questions_dataset[\"ground_truth\"].append(answer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523216e-8a1c-4897-a81a-615bc8481b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f73b4-b05d-4776-a3e5-ab530fff141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(natural_questions_dataset)\n",
    "d.to_csv('research/data/simple_ru_rag_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abedf0-da3a-4f8a-a484-e8e692c67368",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = AnswerCorrectness()\n",
    "res = evaluator.get_correctness(natural_questions_dataset['question'], natural_questions_dataset['answer'], natural_questions_dataset['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a74f3a-ead1-4f44-bd87-fb55fec64a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2e3ca-468e-4b29-97f0-636f20f53c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
