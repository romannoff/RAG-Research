{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90027a7e-95a1-437f-96be-2349debf7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d003ee-243f-439f-903b-063998ed05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4074722-8541-45e6-bf0f-9bdf1b897fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8111d7-6bec-4885-9e03-813b60bba231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rag_techniques.fusion_retrieval import FusionRag\n",
    "from research.functions import get_natural_questions_sample, get_ms_marco_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8389698-90d0-4eb5-9310-c6a456aa3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionRag(collection_name='natural_questions')\n",
    "df = pd.read_csv('research/data/qa_dataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27ce06-68cd-46ef-95aa-bb8e3b080e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_questions_dataset = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "for i, (question, answer) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    result = model(question)\n",
    "    natural_questions_dataset[\"question\"].append(question)\n",
    "    natural_questions_dataset[\"answer\"].append(result['answer'])\n",
    "    natural_questions_dataset[\"contexts\"].append(result['context'])\n",
    "    if pd.isna(answer):\n",
    "        answer = 'No answer'\n",
    "    natural_questions_dataset[\"ground_truth\"].append(answer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af971cbf-9dee-419f-8efb-1532a7ea6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(natural_questions_dataset)\n",
    "new_df.to_csv('research/data/fusion_rag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b8a41-577b-4081-907c-8e7a595070c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.answer_correctness import AnswerCorrectness\n",
    "\n",
    "evaluator = AnswerCorrectness()\n",
    "res = evaluator.get_correctness(natural_questions_dataset['question'], natural_questions_dataset['answer'], natural_questions_dataset['ground_truth'])\n",
    "np.mean(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2cf60-adf2-4a5e-a498-b37ce53ee15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ragas import Evaluator\n",
    "ragas = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d9e3b-cb72-4ddb-870f-7b55644d127d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e = ragas.eval(natural_questions_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf50d98-2bfb-4a82-8e61-152f61e22328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
